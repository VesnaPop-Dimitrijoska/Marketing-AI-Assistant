# MARI - Marketing AI Assistant
A chatbot assistant utilizing Retrieval-Augmented Generation (RAG) to provide intelligent, real-time answers to marketing-related questions, specifically designed for marketing agencies. This assistant can be used in two ways:

* By Marketing Agency Staff: As an internal tool, it helps marketing professionals within the agency quickly access relevant information, optimize campaigns, and manage client queries efficiently, improving their workflow and productivity.

* By Clients or Website Visitors: Deployed on the agency’s website, the chatbot assists clients or potential customers by providing real-time, personalized answers to their marketing-related questions, enhancing user experience and engagement.

In either case, the assistant is a valuable resource for delivering quick and intelligent marketing insights.
#

<div style="text-align: center;">
    <img src="images/Mari - Marketing AI Assistant.webp" alt="Mari - Marketing AI Assistant" width="600"/>
</div>

#

The primary problem that MARI (Marketing AI Assistant) solves is helping individuals navigate the complexities of digital marketing. Many users struggle to understand key marketing concepts like SEO, social media strategies, or website optimization. MARI simplifies these topics by offering clear, personalized, and easy-to-understand answers to their marketing questions. It empowers users to improve their online presence, optimize personal projects, and better manage their digital strategies without needing professional marketing expertise. This makes digital marketing more accessible for everyone.

## Project Overview

This project aims to create a friendly Marketing AI Assistant using a Retrieval-Augmented Generation (RAG) approach. The assistant will be trained on the provided dataset to provide accurate and helpful responses to marketing-related questions.

Key features of the solution include:

- Accessible marketing information: Users can quickly receive answers to their marketing questions anytime and anywhere.
- Consistent responses: The AI assistant ensures consistent information across all interactions.
- Wide-ranging coverage: The system addresses a broad spectrum of marketing topics.
- User-friendly interface: The assistant is designed to be approachable and supportive.

## Data Description

This project utilizes a dataset containing marketing questions and answers. The dataset consists of:

- Question_ID: Unique identifier for each question
- Questions: The actual marketing-related queries
- Answers: Comprehensive explanations and information addressing the questions

You can find the data in [`data/data.csv`](data/data.csv). 

## Technologies

* Python 3.12
* Docker and Docker Compose for containerization
* Minsearch for full-text search
* OpenAI as an LLM
* Streamlit as an web interface. In this project, Streamlit is used to build a chatbot application that: 

    - Allows Users to Ask Questions: Enter questions and receive answers from a marketing knowledge base.
    - Displays Answers: Shows responses generated by the assistant.
    - Collects Feedback: Lets users provide feedback on the answers.


## Running the application

To run the marketing assistant in Jupyter Notebooks, follow these steps:

1. Make sure you have Python 3.12 installed on your system.

2. Copy `.env_template` file into `.env` file and insert your OpenAI API key there. 

3. Install the required dependencies by running the following command in a code cell:
    ```python
    !pip install -r requirements.txt
    ```

3. Run the Jupyter Notebook for experiments.

### Database configuration
Before the application starts for the first time, the database needs to be initialized.

First, run postgres:
```bash
docker-compose up postgres
```

Then run the [`db_prep.py`](marketing_ai_assistant/db_prep.py) script:
```bash
cd marketing_ai_assistant

export POSTGRES_HOST=localhost
python db_prep.py
```

### Running with Docker-Compose

The easiest way to run the application is with docker-compose:
```bash
docker-compose up
```
### Running with Docker (without compose)

Sometimes you might want to run the application in Docker without Docker Compose, e.g., for debugging purposes.

First, prepare the environment by running Docker Compose as in the previous section.

Next, build the image:
```bash
docker build -t marketing_ai_assistant .
```

And run it:
```bash
docker run -it --rm \
  -e OPENAI_API_KEY="${OPENAI_API_KEY}" \
  -e DATA_PATH="data/data.csv" \
  -p 5000:5000 \
  marketing_ai_assistant
```

### Running the Streamlit app

Start the Streamlit interface by running the [`app.py`](marketing_ai_assistant/app.py) script. Open your terminal, navigate to the `marketing_ai_assistant` directory, and execute:

```bash
streamlit run app.py
```

Streamlit will provide a URL in the terminal. Open this URL in your web browser to access the chat interface. You can now enter your questions into the text box, submit them, view the answers from the LLM, and provide feedback.

## Code

The code for the application is in the 
[`marketing_ai_assistant`](marketing_ai_assistant/) folder:

- [`app.py`](marketing_ai_assistant/app.py) - the Streamlit application
- [`ingest.py`](mmarketing_ai_assistant/ingest.py) - loading the data into the knowledge base
- [`minsearch.py`](marketing_ai_assistant/minsearch.py) - an in-memory search engine
- [`rag.py`](marketing_ai_assistant/rag.py) - the main RAG logic for building the retrieving the data and building the prompt 
- [`db.py`](marketing_ai_assistant/db.py) - the logic for logging the requests and responses to postgres
- [`db_prep.py`](marketing_ai_assistant/db_prep.py) - the script for initializing the database
 

## Interface
I used Streamlit for serving the application.

Refer to "Running the Streamlit app" section for more detail.


## Ingestion
The ingestion script is in [`ingest.py`](marketing_ai_assistant/ingest.py).

Since I used an in-memory database, `minsearch`, as our knowledge base, I runed the ingestion script at the startup of the application.

It's executed inside [`rag.py`](marketing_ai_assistant/rag.py) when I import it.

## Experiments

For experiments, we use Jupyter notebooks. They are in the [`notebooks`](notebooks/) folder.

For the code for evaluating the system, you can check the [`notebooks/rag-test.ipynb`](notebooks/rag-test.ipynb) noteboook.


Notebooks:

* [`rag-test.ipynb`](notebooks/rag-test.ipynb): the RAG flow and evaluating
* [`evaluation-data-generation.ipynb`](notebooks/evaluation-data-generation.ipynb): generating the ground truth dataset for retrieval evaluation

### Retrieval evaluation

The basic approach - using `minsearch` without any boosting - gave the following metrics:

* hit_rate: 98%
* MRR:      93% 

The the attempt for improving the result was not succesfull, I received the same result:

* hit_rate: 98%
* MRR:      93% 

The boosting parameters used are the following: 

```python
    boost = {
        'Question_ID':  0.77,
        'Question':     1.41,
        'Answer':       1.44
    }
```

### RAG flow evaluation

I used the LLM as a Judge metric to evaluate the quality of the RAG flow.

I Evaluation was made with gpt-4o-mini, the result among 100 sample records:

* 58 (58%) RELEVANT
* 25 (25%) PARTLY_RELEVANT
* 17 (17%) IRRELEVANT

II Evaluation was made with gpt-4o, the result among 100 sample records:

* 49 (49%) RELEVANT
* 30 (30%) PARTLY_RELEVANT
* 21 (21%) IRRELEVANT


## Monitoring
[LangFuse](https://cloud.langfuse.com) is a callback handler for logging and monitoring performance. It helps us to visualize and analyze our application's traces effectively. 

So, to have access to LangFuse traces, create an account [here](https://cloud.langfuse.com/auth/sign-up). Then create a new project and then create new API credentials in the project settings. Add the secret key, the public key, and langfuse host to your environment variables as LANGFUSE_SECRET_KEY; LANGFUSE_PUBLIC_KEY; and LANGFUSE_HOST respectively.

You can access the LangFuse dashboard by logging into your account at [LangFuse](https://cloud.langfuse.com/auth/sign-up).

At the Langfuse dashboard we can see the `traces` that record every interaction with the model and `model costs` which refer to the costs required to operate and maintain the model. We can also see:
- `Trace Latencies`: This is the time taken to process and respond to individual requests. It helps us gauge the responsiveness of the model.
- `Generation Latencies`: This metric tracks the total time taken by the model to generate a response after receiving a request. It’s crucial for assessing how quickly the model delivers results.
- `Model Latencies`: This represents the overall time for the model to process a request and produce a response. It’s an important indicator of the model’s overall efficiency.

### Langfuse Dashboards
![alt text](<images/Screenshot from 2024-09-09 23-18-38.png>)
![alt text](<images/Screenshot from 2024-09-09 23-19-15.png>)
![alt text](<images/Screenshot from 2024-09-09 23-19-53.png>)
![alt text](<images/Screenshot from 2024-09-09 23-20-22.png>)
![alt text](<images/Screenshot from 2024-09-09 23-20-30.png>)
![alt text](<images/Screenshot from 2024-09-09 23-20-34.png>)
![alt text](<images/Screenshot from 2024-09-09 23-20-40.png>)

